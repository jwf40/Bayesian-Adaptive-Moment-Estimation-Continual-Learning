BBBLinear()
BBBLinear()
-- >> Start of training phase << --
SKIPPING RESET OPTIMIZER!!!!
0it [00:00, ?it/s]
Traceback (most recent call last):
  File "main.py", line 99, in <module>
    cl_strategy.train(train_exp)
  File "/usr/local/lib/python3.8/dist-packages/avalanche/training/templates/base_sgd.py", line 146, in train
    super().train(experiences, eval_streams, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/avalanche/training/templates/base.py", line 116, in train
    self._train_exp(self.experience, eval_streams, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/avalanche/training/templates/base_sgd.py", line 264, in _train_exp
    self.training_epoch(**kwargs)
  File "/tf/notebooks/Labelled Experiments/badam.py", line 128, in training_epoch
    self.backward()
  File "/usr/local/lib/python3.8/dist-packages/avalanche/training/templates/base_sgd.py", line 200, in backward
    self.loss.backward()
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.